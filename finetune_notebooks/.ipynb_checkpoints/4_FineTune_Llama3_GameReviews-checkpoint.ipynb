{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd272cf0-eb00-4faa-8fa3-914654e2156f",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb503fd-375f-4a11-a8d2-57c712f4a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "###\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "###\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a28c5ea-c8d9-43ba-b964-65d72638cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b40fc4-d088-47c1-a4c0-b550835706ac",
   "metadata": {},
   "source": [
    "# Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55b9d7-244d-4e42-a6c2-626b613b7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 #2048 \n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "model_dtype = torch.float16\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\", #\"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #dtype = dtype,\n",
    "    dtype = model_dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bdbd24-061a-4a3d-ae0c-8105553d2885",
   "metadata": {},
   "source": [
    "# LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc07635-d388-4d98-9ee5-c2d707acf3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                     \"embed_tokens\", \"lm_head\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d99b9d1-2aa3-493a-bc20-25e8cc391f5f",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc8602c-1f44-4c23-9e66-ab4f9f598f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import json\n",
    "from functools import partial\n",
    "def format_prompt(example, tokenizer):\n",
    "    try:\n",
    "        target_json_str = json.dumps(example['target_json_output'], ensure_ascii=False)\n",
    "    except TypeError as e:\n",
    "        print(f\"Error converting target_json_output to string: {e}\")\n",
    "        # Add more context if 'id' is part of your example from Label Studio\n",
    "        # print(f\"Problematic example ID (if available): {example.get('id')}\")\n",
    "        target_json_str = \"{}\" # Fallback to empty JSON string\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": example.get('system_prompt', \"You are a helpful assistant.\")},\n",
    "        {\"role\": \"user\", \"content\": example['user_review']},\n",
    "        {\"role\": \"assistant\", \"content\": f\"```json\\n{target_json_str}\\n```\"} # This is the target JSON output\n",
    "    ]\n",
    "    # apply_chat_template returns the formatted string if tokenize=False\n",
    "    formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "\n",
    "def _tokenize_function(examples, tokenizer):\n",
    "    output = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "        return_attention_mask=True,\n",
    "        add_special_tokens=False # fix double tokens at beginning?\n",
    "    )\n",
    "    tokenized_inputs = {\n",
    "        'input_ids': output['input_ids'],\n",
    "        'attention_mask': output['attention_mask'],\n",
    "    }\n",
    "    tokenized_inputs[\"labels\"] = output[\"input_ids\"].copy()\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    "def create_training_dataset(jsonl_path, tokenizer, test_size = 0.2, seed=42):\n",
    "\n",
    "    if tokenizer.chat_template is None:\n",
    "        print(\"WARNING: tokenizer.chat_template is not set. Manually applying Llama 3.1 template.\")\n",
    "        tokenizer.chat_template = (\n",
    "            \"{% set loop_messages = messages %}\"\n",
    "            \"{% for message in loop_messages %}\"\n",
    "                \"{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\"\n",
    "                \"{% if loop.index0 == 0 %}\"\n",
    "                    \"{{ '<|begin_of_text|>' + content }}\"\n",
    "                \"{% else %}\"\n",
    "                    \"{{ content }}\"\n",
    "                \"{% endif %}\"\n",
    "            \"{% endfor %}\"\n",
    "        )\n",
    "        # Also ensure padding token is set when manually setting chat_template\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(f\"Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
    "    else:\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            print(f\"Tokenizer pad_token set to eos_token: {tokenizer.pad_token}\")\n",
    "    \n",
    "    print(f\"Loading data from: {jsonl_path}\")\n",
    "    raw_dataset = load_dataset(\"json\", data_files=jsonl_path)\n",
    "    if 'train' not in raw_dataset:\n",
    "        print(\"Error: dataset does not contain 'train' split. check jsonl structure\")\n",
    "        return None\n",
    "    print(f\"Dataset loaded with {len(raw_dataset['train'])} samples.\")\n",
    "\n",
    "    format_func_with_tokenizer = partial(format_prompt, tokenizer=tokenizer)\n",
    "\n",
    "    \n",
    "    print(\"Applying Llama 3.1 chat template formatting ...\")\n",
    "    formatted_dataset = raw_dataset.map(\n",
    "        format_func_with_tokenizer, \n",
    "        remove_columns=['system_prompt', 'user_review', 'target_json_output'],\n",
    "        num_proc=4 # Use multiple processes for faster mapping\n",
    "    )\n",
    "    #print(f\"DEBUG: Features after formatting step: {formatted_dataset['train'].features}\")\n",
    "    tokenize_func_with_tokenizer = partial(_tokenize_function, tokenizer=tokenizer)\n",
    "\n",
    "    print(\"Tokenizing the formatted dataset ...\")\n",
    "    tokenized_dataset = formatted_dataset.map(\n",
    "        tokenize_func_with_tokenizer, # Use the partial function here\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "        num_proc=4\n",
    "    )\n",
    "    #print(f\"DEBUG: Features after tokenization step: {tokenized_dataset['train'].features}\")\n",
    "\n",
    "    print(f\"Splitting dataset into train ({1-test_size:.0%}) and test ({test_size:.0%})...\")\n",
    "    # Perform the train-test split\n",
    "    split_dataset = tokenized_dataset['train'].train_test_split(test_size=test_size, seed=seed)\n",
    "\n",
    "    return DatasetDict({\n",
    "        'train': split_dataset['train'],\n",
    "        'test': split_dataset['test']\n",
    "    })\n",
    "\n",
    "\n",
    "train_filepath = './training_data.jsonl'\n",
    "dataset = create_training_dataset(train_filepath, tokenizer, test_size=0.0)\n",
    "\n",
    "if dataset:\n",
    "    print(\"\\nDataset successfully created and split:\")\n",
    "    print(dataset)\n",
    "    #print(\"\\nFeatures of the training dataset:\")\n",
    "    #print(dataset['train'].features)\n",
    "    print(\"\\nFirst training example:\")\n",
    "    # Decode to verify the actual text that will be fed to the model\n",
    "    decoded_text_unnested = tokenizer.decode(dataset['train'][0]['input_ids'])\n",
    "    print(decoded_text_unnested)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80886bed-06c2-44e1-bb33-df39559341b4",
   "metadata": {},
   "source": [
    "# Config Model Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f617a6d-a3aa-47ab-9c55-ea62b1cf6ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'], \n",
    "    #eval_dataset = dataset['test'], \n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #dataset_num_proc = 2,\n",
    "    dataset_num_proc = None,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 10, # (3-5 for 1k) The number of training epochs(0 if the maximum steps are defined) 1\n",
    "        max_steps = -1,  # The maximum steps (0 if the epochs are defined) 130\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        #output_dir = \"outputs\",\n",
    "        output_dir = \"/content/drive/MyDrive/my_llama_finetune_checkpoints\",\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 50,\n",
    "        save_total_limit = 2,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8e6dc-f1de-410c-a490-7afad7fcade5",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3944bf-324b-4f23-9266-dfbc548eeec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7ef895-5c6a-427c-9a43-f6433b59c004",
   "metadata": {},
   "source": [
    "# Save to HF (multiple options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d23f23-255c-4a9a-8234-e66c53565c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"GameReview-llama3.1-8b-v1\"\n",
    "hf_username = \"MrMike42\"\n",
    "hf_username_model = f\"{hf_username}/{modelname}\"\n",
    "\n",
    "\n",
    "# Merge then convert to GGUF then PUSH\n",
    "# Very slow, needs lots of disk space, memory and GPU\n",
    "# Completely finshes process\n",
    "if False:\n",
    "    print(f\"Starting GGUF export and push to: {hf_username_modelname\")\n",
    "    model.push_to_hub_gguf(f\"{hf_username_model}\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "    print(\"done\")\n",
    "   \n",
    "\n",
    "# push lora - FAST, but merging with base model and distill to GGUF still needed\n",
    "# use this to quickly save work\n",
    "if False:\n",
    "# Use a descriptive name for your ADAPTERS repository to distinguish it from the GGUF model\n",
    "    adapter_model_name = f\"{modelname}-adapters\"\n",
    "    print(f\"Pushing ONLY LoRA adapters to Hugging Face Hub: {hf_username_model}\")\n",
    "    # This pushes the small adapter weights, not the large GGUF\n",
    "    trainer.push_to_hub(f\"{hf_username_model}\")\n",
    "    print(\"done\")\n",
    "    \n",
    "      \n",
    "\n",
    "# push 16bit model from hosted directory - still needs to convert to GGUF\n",
    "# this is case where model is no longer in memory\n",
    "# (BUILT MODEL ON KAGGLE THEN RAN OUT OF DISK TRYING TO PUSH TO HF)\n",
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import torch\n",
    "    import os\n",
    "    \n",
    "    # Assuming your merged model is in this directory\n",
    "    local_model_directory = \"/kaggle/working/MrMike42/GameReview-llama3.1-8b-v2-GGUF\" \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Attempting to load model onto: {device}\")\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = local_model_directory,\n",
    "        max_seq_length = 1024, # Use your original max_seq_length\n",
    "        dtype = torch.float16, # Auto-detect, or torch.float16 if it was saved as 16bit\n",
    "        load_in_4bit = False, # If you want to push the full 16-bit model\n",
    "        device_map = device,\n",
    "        )\n",
    "\n",
    "    tokenizer = FastLanguageModel.from_pretrained(local_model_directory).tokenizer\n",
    "    repo_id = hf_username_model\n",
    "    model.push_to_hub(repo_id, token=True) # token=True uses the logged-in token\n",
    "    tokenizer.push_to_hub(repo_id, token=True)\n",
    "    print(f\"Model successfully pushed to: https://huggingface.co/{repo_id}\")\n",
    "\n",
    "\n",
    "# push 16bit model from memory - still needs to convert to GGUF\n",
    "# this is the case where the model is still in memory\n",
    "if False:\n",
    "    model.push_to_hub_merged(f\"{hf_username_model}\", tokenizer, save_method = \"merge_16bit\", token=True)\n",
    "    print(f\"Model successfully pushed to: https://huggingface.co/{repo_id}\")  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814e9c89-2652-4e03-b3db-1b82b3830ecb",
   "metadata": {},
   "source": [
    "# Save the trainer stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc51997-0450-4b47-985b-02a723744383",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trainer_stats.json\", \"w\") as f:\n",
    "    json.dump(trainer_stats, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f1850-7bf4-49b1-852b-75c6aca19c06",
   "metadata": {},
   "source": [
    "# Save the finetuned model & push to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786335fe-61c6-43d8-8b16-06d4aa98605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 4bit\n",
    "modelname = \"GameReview-llama3.1-8b-v1\"\n",
    "hf_username = \"MrMike42\"\n",
    "hf_username_model = f\"{hf_username}/{modelname}\"\n",
    "#model.save_pretrained_gguf(modelname, tokenizer, quantization_method = \"q4_k_m\")\n",
    "#trainer.push_to_hub(\"MrMike42/GameReview-llama3.1-8b-instruct-adapters\")\n",
    "model.push_to_hub_gguf(hf_username_model, tokenizer, quantization_method = \"q4_k_m\")#, token = \"\") #\"hf_username/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eeb4a6-a49d-4fef-9137-8f222a6189a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving JUST the whole model\n",
    "\n",
    "from huggingface_hub import HfApi, notebook_login, create_repo\n",
    "import os\n",
    "local_model_directory = \"/kaggle/working/MrMike42/GameReview-llama3.1-8b-v2-GGUF\" # <--- IMPORTANT: Adjust this to your actual directory\n",
    "repo_id = \"MrMike42/GameReview-llama3.1-8b-v2-direct-upload\" # <--- IMPORTANT: Replace with your actual username and desired repo name\n",
    "# Initialize HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# 1. Create the repository on the Hugging Face Hub\n",
    "# This step is important if the repo doesn't exist. `exist_ok=True` prevents errors if it already does.\n",
    "try:\n",
    "    create_repo(repo_id=repo_id, private=False, exist_ok=True, repo_type=\"model\")\n",
    "    print(f\"Repository '{repo_id}' created or already exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating/checking repository: {e}\")\n",
    "    # If it's an authentication error, notebook_login() might need to be re-run\n",
    "    # or token might be expired/invalid.\n",
    "\n",
    "# 2. Upload the entire folder to the repository\n",
    "print(f\"Uploading files from '{local_model_directory}' to '{repo_id}' without loading model into RAM...\")\n",
    "api.upload_folder(\n",
    "    folder_path=local_model_directory,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\", # Specify it's a model repository\n",
    "    commit_message=\"Upload merged 16-bit Llama 3.1 8B model (direct file upload)\",\n",
    "    # Use multi_commits=True for very large uploads to help with network stability\n",
    "    # and resumption if an error occurs.\n",
    "    \n",
    ")\n",
    "\n",
    "print(f\"Model files successfully uploaded to: https://huggingface.co/{repo_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b29d6-ddcf-4e3d-8c7a-2882cd6515b0",
   "metadata": {},
   "source": [
    "# Run the model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7783021-2063-4938-b371-2e99e66d38a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06f48ae-4fd1-48d6-af1c-e34a05f2858e",
   "metadata": {},
   "source": [
    "#DOWNLOAD VIA OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a8416-7e9e-4a2f-917e-25249613fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ollama run hf.co/{username}/{repository}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b463845d-1e99-45b2-81c8-719b08cb5aec",
   "metadata": {},
   "source": [
    "Yes. If I have a bigger dataset, I would lower the learning rate to maximize chance of not settling in a local minimum. If you have small dataset, you want to do more epochs on it with big global batch size to limit overfitting. If you use sample packing, remember to bump up learning rate or you will effectively go over let's say 5x as many samples but have the same lr and steps, so effective learning rate per sample will be lower. If you use sample_packing and have short 100/200 token samples, this will be especially important.\n",
    "\n",
    "Even when I not following best practices, for example by using global batch size 1 and high learning rate, my results were usually fine, so I don't think hyperparameters are very important for finetuning unless you majorly screw them up. Dataset is much more important. \n",
    "\n",
    "Dataset for DPO/ORPO should be at least 1000 samples and for SFT I would say over 5000 samples. Watch out for any biases in DPO/ORPO dataset that you may not want to have in final model - one good example is unalignment/toxic-dpo-0.1 - it contains a lot of numbered lists. A model trained on it, if you have a high enough learning rate/epoch count, will love to output numbered lists even in informal chats.\n",
    "\n",
    "Its good to check current tech you're using periodically and switch to new better solutions. Unsloth, ORPO, GaLore, rslora, loftq+ - every few weeks a new thing comes out that can improve your finetune quality basically for free if you just incorporate it in the training.\n",
    "\n",
    "\n",
    "5000 pairs should be enough for most tasks and I would say 1 or 2 epochs should be enough to improve on a base model.\n",
    "\n",
    "If you have a small dataset (under 5k for sft) and you don't want to overfit, i think you should go with big batch size, high epochs and medium learning rate. So let's say 500 samples, 64 batch size, 30 epochs, 0.00005 lr (lr is very model dependant).\n",
    "\n",
    "If you instead go for 500 samples, 4 batch size, 1 epoch, 0.0002 lr i think you are more likely to undershoot or overfit. It can still work, but I think there is more of a chance it won't. Each step has a much higher learning rate and less samples included, so you're more likely to train a model that does not converge to the global minimum.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce100553-9a0e-4fdb-a1f9-b31874defaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
