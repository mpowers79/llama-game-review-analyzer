{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c9f287-5ca0-4201-8340-7c77cdc5e73c",
   "metadata": {},
   "source": [
    "# Gemini-assisted prelabeling of LLM training data\n",
    "\n",
    "**Purpose:**  Use Gemini to prelabel LLM training data. Store output in JSON for importing into Label Studio.\n",
    "Prompt and JSON format customized for extracting sentiment and keywords from game reviews.\n",
    "\n",
    "---\n",
    "**Copyright (c) 2025 Michael Powers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed86a5-c9a0-44a6-a0a9-f90a06db37fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38b88a-9d56-4444-b981-bcc95241b761",
   "metadata": {},
   "source": [
    "# Gemini Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0748274b-0d59-46b5-a325-808d3b6227db",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = 'models/gemini-2.0-flash-lite'\n",
    "api_key=\"YOUR GEMINI API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e93d141-0328-4e26-96b0-ef862efd672b",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0ab0de-a9f9-4a38-9a5d-a7097a56928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = \"You are an expert game review analyzer. Your task is to extract structured information from game reviews, outputting a precise JSON object with sentiment, specific keywords, and negative flags. Ensure the output is valid JSON, following this schema: {'sentiment': {'overall': 'positive|negative|neutral|mixed', 'recommendation': true|false, 'warning_anti_recommendation': true|false}, 'specifics': {'positive_keywords': ['list', 'of', 'phrases'], 'negative_keywords': ['list', 'of', 'phrases']}, 'negative_tracker': {'ad_game_mismatch': true|false, 'game_cheating_manipulating': true|false, 'bugs_crashes_performance': true|false, 'monetization': true|false, 'live_ops_events': true|false}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c7ecc4-f2c7-4ba6-994a-f4ce6a4c4263",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed1c11f-edd0-46b3-b793-9cd01902eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(prompt, review):\n",
    "    return f'{prompt}\\n\\nREVIEW: {review}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755824c0-d0b9-4dd4-8351-db6c20f3760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini_json(prompt, use_json=True, model='models/gemini-2.0-flash-lite'):\n",
    "    import os\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(model)\n",
    "    if use_json:\n",
    "        generation_config = genai.GenerationConfig(response_mime_type=\"application/json\")\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "    else:\n",
    "        response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e101b-aa11-44f6-8fbc-7b661bcb7028",
   "metadata": {},
   "source": [
    "# Assemble into Label Studio compatable JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef085fef-3a76-4b79-ae71-0d91bfc0a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_studio_prediction(system_prompt_text, user_review_text, llm_output_json_string):\n",
    "    \"\"\"\n",
    "    Creates a Label Studio prediction structure from your LLM's output.\n",
    "\n",
    "    Args:\n",
    "        system_prompt_text (str): The system prompt text.\n",
    "        user_review_text (str): The user review text.\n",
    "        llm_output_json_string (str): The LLM's raw JSON string output.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary formatted for Label Studio import, with data and predictions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        llm_parsed_output = json.loads(llm_output_json_string)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON string provided for LLM output: {llm_output_json_string}\")\n",
    "        return None\n",
    "\n",
    "    # Helper to get boolean values as 'true'/'false' strings\n",
    "    def get_bool_choice(val):\n",
    "        return 'true' if val else 'false'\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Sentiment\n",
    "    sentiment = llm_parsed_output.get('sentiment', {})\n",
    "    if 'overall' in sentiment:\n",
    "        results.append({\n",
    "            \"from_name\": \"sentiment_overall\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"choices\",\n",
    "            \"value\": {\"choices\": [sentiment['overall']]}\n",
    "        })\n",
    "    if 'recommendation' in sentiment:\n",
    "        results.append({\n",
    "            \"from_name\": \"sentiment_recommendation\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"choices\",\n",
    "            \"value\": {\"choices\": [get_bool_choice(sentiment['recommendation'])]}\n",
    "        })\n",
    "    if 'warning_anti_recommendation' in sentiment:\n",
    "        results.append({\n",
    "            \"from_name\": \"sentiment_warning\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"choices\",\n",
    "            \"value\": {\"choices\": [get_bool_choice(sentiment['warning_anti_recommendation'])]}\n",
    "        })\n",
    "\n",
    "    # Specifics\n",
    "    specifics = llm_parsed_output.get('specifics', {})\n",
    "    if 'positive_keywords' in specifics:\n",
    "        results.append({\n",
    "            \"from_name\": \"specifics_positive_keywords\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"textarea\",\n",
    "            \"value\": {\"text\": \", \".join(specifics['positive_keywords'])} # Join list back into string\n",
    "        })\n",
    "    if 'negative_keywords' in specifics:\n",
    "        results.append({\n",
    "            \"from_name\": \"specifics_negative_keywords\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"textarea\",\n",
    "            \"value\": {\"text\": \", \".join(specifics['negative_keywords'])} # Join list back into string\n",
    "        })\n",
    "\n",
    "    # Negative Tracker\n",
    "    negative_tracker = llm_parsed_output.get('negative_tracker', {})\n",
    "    if 'ad_game_mismatch' in negative_tracker:\n",
    "        results.append({\n",
    "            \"from_name\": \"nt_ad_mismatch\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"choices\",\n",
    "            \"value\": {\"choices\": [get_bool_choice(negative_tracker['ad_game_mismatch'])]}\n",
    "        })\n",
    "    if 'game_cheating_manipulating' in negative_tracker:\n",
    "        results.append({\n",
    "            \"from_name\": \"nt_cheating_manipulating\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"choices\",\n",
    "            \"value\": {\"choices\": [get_bool_choice(negative_tracker['game_cheating_manipulating'])]}\n",
    "        })\n",
    "    if 'bugs_crashes_performance' in negative_tracker:\n",
    "        results.append({\n",
    "            \"from_name\": \"nt_bugs_crashes_performance\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"choices\",\n",
    "            \"value\": {\"choices\": [get_bool_choice(negative_tracker['bugs_crashes_performance'])]}\n",
    "        })\n",
    "    if 'monetization' in negative_tracker:\n",
    "        results.append({\n",
    "            \"from_name\": \"nt_monetization\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"choices\",\n",
    "            \"value\": {\"choices\": [get_bool_choice(negative_tracker['monetization'])]}\n",
    "        })\n",
    "    if 'live_ops_events' in negative_tracker:\n",
    "        results.append({\n",
    "            \"from_name\": \"nt_live_ops_events\",\n",
    "            \"to_name\": \"user_review_text\",\n",
    "            \"type\": \"choices\",\n",
    "            \"value\": {\"choices\": [get_bool_choice(negative_tracker['live_ops_events'])]}\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"data\": {\n",
    "            \"system_prompt\": system_prompt_text,\n",
    "            \"user_review\": user_review_text\n",
    "        },\n",
    "        \"predictions\": [\n",
    "            {\n",
    "                \"model_version\": \"your_llm_model_name_vX\", # Give your model a version for tracking\n",
    "                \"result\": results\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d12d304-f665-4e51-900b-a9d65f107b27",
   "metadata": {},
   "source": [
    "# Write a Label Studio JSON Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880073f6-6fe5-44b3-9f48-fc14cdd61b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_review_to_json(task, filepath=\"./tasks_with_predictions.json\"):\n",
    "    if not task:\n",
    "        print(\"Prediction was empty\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(filepath) and os.path.getsize(filepath) > 0:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            if not isinstance(data, list):\n",
    "                data = [data]\n",
    "        except json.JSONDecodeError:\n",
    "            data = []\n",
    "    else:\n",
    "        data = []\n",
    "\n",
    "    data.append(task)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    print(\"--Prediction added to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b775b-6984-4366-9821-76583c866ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d730e-a8d1-4870-912f-0ee0281a4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews(file_path=\"./game_reviews.txt\", out_path=\"./tasks_with_predictions.json\"):\n",
    "    delimiter = \"---END_SNIPPET---\"\n",
    "\n",
    "    #with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    snippets = content.split(delimiter)\n",
    "\n",
    "    # Clean up empty strings that might result from trailing delimiters or initial empty content\n",
    "    #snippets = [snippet.strip() for snippet in snippets if snippet.strip()]\n",
    "\n",
    "    #get response from LLM, put in right format, write to file\n",
    "    for i, snippet in enumerate(snippets):\n",
    "        print(f'Processing review: {i+1}')\n",
    "        response = ask_gemini_json(build_prompt(base_prompt, snippet))\n",
    "        task_with_prediction = create_label_studio_prediction(base_prompt, snippet, response)\n",
    "        add_review_to_json(task_with_prediction, out_path)\n",
    "    print(\"--------------------------------------------\")\n",
    "    print(f\"Done processing file: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca9f676f-1039-43f8-b2b0-6e11b75c09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_reviews_rate_limited(input_filepath=\"./game_reviews.txt\", \n",
    "                                output_filepath=\"./predictions.json\",\n",
    "                                progress_filepath=\"./gamereview_progress.txt\",\n",
    "                                num_max=10, \n",
    "                                model='models/gemini-2.0-flash-lite'):\n",
    "\n",
    "    delimiter = \"---END_SNIPPET---\"\n",
    "    #prompt_template = augmentation_prompt_template\n",
    "    RPM_LIMIT = 15\n",
    "    MAX_RETRIES = 5\n",
    "    BASE_SLEEP_TIME = 4.5\n",
    "    start_from_line = 0\n",
    "    reviews_processed = 0\n",
    "    \n",
    "    try:\n",
    "        with open(progress_filepath, 'r') as f:\n",
    "            start_from_line = int(f.read().strip())\n",
    "            print(f\"Resuming. Starting from line {start_from_line} in the input file.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Progress file not found or empty. Starting from the beginning.\")\n",
    "        pass \n",
    "\n",
    "    session_reviews_processed = 0\n",
    "    print(f\"Starting generation. Goal: Process up to {num_max} reviews.\")\n",
    "\n",
    "    # Counter for API calls made within the current minute\n",
    "    requests_in_minute = 0\n",
    "    start_time_minute = time.time()\n",
    "\n",
    "    with open(input_filepath, \"r\") as f:\n",
    "        content = f.read()\n",
    "    snippets = content.split(delimiter)\n",
    "\n",
    "    for i, snippet in enumerate(snippets):\n",
    "        if i < start_from_line:\n",
    "            continue\n",
    "        if num_max > 0 and session_reviews_processed >= num_max:\n",
    "            print(f'Done: Reached processing limit of {num_max} original reviews for this session.')\n",
    "            break  \n",
    "\n",
    "        formatted_prompt = build_prompt(base_prompt, snippet)\n",
    "        retries = 0\n",
    "        \n",
    "        while retries < MAX_RETRIES:\n",
    "            # Check RPM limit\n",
    "            current_time = time.time()\n",
    "            if current_time - start_time_minute >= 60:\n",
    "                requests_in_minute = 0\n",
    "                start_time_minute = current_time\n",
    "\n",
    "            if requests_in_minute >= RPM_LIMIT:\n",
    "                wait_time = 60 - (current_time - start_time_minute)\n",
    "                print(f\"Rate limit hit. Waiting for {wait_time:.2f} seconds...\")\n",
    "                time.sleep(wait_time + 1)\n",
    "                requests_in_minute = 0\n",
    "                start_time_minute = time.time()\n",
    "\n",
    "            \n",
    "            try: # LLM CALL\n",
    "                print(f'Making LLM Call: {session_reviews_processed}')\n",
    "                response = ask_gemini_json(formatted_prompt, use_json=True, model=model)\n",
    "                requests_in_minute += 1\n",
    "                \n",
    "                # Clean the response string\n",
    "                response = response.strip()\n",
    "                if response.startswith(\"```json\") and response.endswith(\"```\"):\n",
    "                    response = response[len(\"```json\"): -len(\"```\")].strip()\n",
    "                elif response.startswith(\"```\") and response.endswith(\"```\"):\n",
    "                    response = response[len(\"```\"): -len(\"```\")].strip()\n",
    "\n",
    "                task_with_prediction = create_label_studio_prediction(base_prompt, snippet, response)\n",
    "                add_review_to_json(task_with_prediction, output_path)\n",
    "                reviews_processed += 1\n",
    "                session_reviews_processed += 1\n",
    "                with open(progress_filepath, 'w') as prog_f:\n",
    "                    prog_f.write(str(i + 1))\n",
    "\n",
    "                break # Success, break out of retry loop\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                sleep_duration = BASE_SLEEP_TIME * (2 ** (retries - 1)) + random.uniform(0, 1)\n",
    "                print(f\"API Error for input line {i}: {e}. Retrying in {sleep_duration:.2f}s... (Attempt {retries}/{MAX_RETRIES})\")\n",
    "                time.sleep(sleep_duration)\n",
    "\n",
    "            if retries == MAX_RETRIES:\n",
    "                print(f\"Failed to process review from input line {i} after {MAX_RETRIES} retries. Skipping.\")\n",
    "                # We still update the progress file to avoid getting stuck on a failing entry\n",
    "                with open(progress_filepath, 'w') as prog_f:\n",
    "                    prog_f.write(str(i + 1))\n",
    "            \n",
    "    print(\"------DONE-----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8511d5-1c93-4967-9ad5-4b9f187ef7bd",
   "metadata": {},
   "source": [
    "# Run to get first pass reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a902cec-a2e2-4f2c-867e-f252f65a5951",
   "metadata": {},
   "source": [
    "**Note: This code does not handle rate limiting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79837aa1-9ba5-47e5-b801-6a61a92465c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_reviews(\"./game_reviews.txt\", \"./predictions_for_label_studio_import.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca330a9-5dd4-4c18-9c0f-2e861aefe0cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1fd0dc-6d10-4a1c-9478-4eafa3fba39f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
