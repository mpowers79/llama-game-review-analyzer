{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b070d5f-2f2b-4b9e-ae5f-9e6677a3274a",
   "metadata": {},
   "source": [
    "# Fine-tuned LLM Scoring\n",
    "\n",
    "**Purpose:**  Test fine-tuned LLMs with JSONL test data against a custom scoring function.\n",
    "Has ability to test the fine-tuned LLM, a baseline model, and a Gemini model (a baseline commercial model)\n",
    "Scoring function customized to evaluate specialized JSON LLM response.\n",
    "Scores can optionally be exported to .csv\n",
    "\n",
    "---\n",
    "**Copyright (c) 2025 Michael Powers.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72b58f-adab-4fb8-b490-c4c636fb0094",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb697b1-8b14-4cc3-a7b9-908729ee1e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "from typing import List, Dict, Any, Optional, Callable, Awaitable\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# LlamaIndex imports\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.llms import ChatMessage, MessageRole, ChatResponse\n",
    "from llama_index.core.evaluation import EvaluationResult, BaseEvaluator\n",
    "\n",
    "# Google Generative AI direct import\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dba0f57-13b1-4fa7-a22d-c8645c5cfd93",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33286796-a397-4d4a-bc24-58049dab3fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_my_jsonl_data(file_path) -> List[dict]:\n",
    "    data = []\n",
    "    with open(file_path, 'r' ) as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line.strip())\n",
    "            data.append(item)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde2a95-8c37-49dc-8be1-e016536114c2",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea663cc-539c-4fb9-9e7b-f62ddc303ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat_messages(data_item: Dict) -> List[ChatMessage]:\n",
    "    \"\"\"Creates a list of ChatMessage objects for the LLM.\"\"\"\n",
    "    messages = [\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=data_item[\"system_prompt\"]),\n",
    "        ChatMessage(role=MessageRole.USER, content=data_item[\"user_review\"])\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d244213-8344-4d9a-871e-00c99d2301b9",
   "metadata": {},
   "source": [
    "# LLM Callers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b64a4-92eb-475e-9103-bf30b0187341",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = 'models/gemini-2.0-flash-lite'\n",
    "api_key=\"YOUR_API_KEY\"\n",
    "def ask_gemini_json(messages, gemini_model):\n",
    "    import os\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(gemini_model)\n",
    "    generation_config = genai.GenerationConfig(response_mime_type=\"application/json\")\n",
    "\n",
    "    system_instruction = None\n",
    "    user_message = None\n",
    "    \n",
    "    for i, msg in enumerate(messages):\n",
    "        if msg.role == MessageRole.SYSTEM:\n",
    "            system_instruction = msg.content\n",
    "        elif msg.role == MessageRole.USER:\n",
    "            user_message = msg.content\n",
    "    prompt = f'{system_instruction}\\n {user_message}'\n",
    "\n",
    "    response = model.generate_content(prompt, generation_config=generation_config)\n",
    "    #response = chat.send_message(current_user_message_content, generation_config=generation_config)\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9500653-4856-41b0-a669-58bd0b924768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama3_json(messages, model_name):\n",
    "    from llama_index.core.chat_engine import SimpleChatEngine\n",
    "    from llama_index.core import Settings\n",
    "    Settings.llm = Ollama(model=model_name, request_timeout=10000.0, temperature = 0.0, json_mode=True)\n",
    "    system_prompt = \"\"\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == MessageRole.SYSTEM:\n",
    "            system_prompt = message.content\n",
    "        if message.role == MessageRole.USER:\n",
    "            prompt = message.content\n",
    "    wrapped_system_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> {system_prompt} <|eot_id|>\"\n",
    "    wrapped_prompt = f\"<|start_header_id|>user<|end_header_id|>{prompt}<|eot_id|>\"\n",
    "\n",
    "    if True:\n",
    "        print(f'---System Prompt: {wrapped_system_prompt}')\n",
    "        print(f'---prompt: {wrapped_prompt}')\n",
    "    \n",
    "    chat_engine = SimpleChatEngine.from_defaults(system_prompt = wrapped_system_prompt)\n",
    "    response = chat_engine.chat(wrapped_prompt)\n",
    "    return response.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e65703b-4efa-45de-89c6-c1af420e1cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3185c-56b7-4c65-a1d3-90c3b4aa076a",
   "metadata": {},
   "source": [
    "# Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ac8923-3585-43f0-b45f-48ea011aae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_for_keywords_embedding_lenient(true_keywords_list, pred_keywords_list, embed_model=embed_model, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Calculates F1 score for keyword extraction with lenient matching using embeddings.\n",
    "    \n",
    "    Args:\n",
    "        true_keywords_list (list): List of ground truth keywords.\n",
    "        pred_keywords_list (list): List of predicted keywords.\n",
    "        embed_model: An initialized LlamaIndex embedding model (e.g., OllamaEmbedding).\n",
    "        similarity_threshold (float): Minimum cosine similarity score (0.0 to 1.0) for a match.\n",
    "    \n",
    "    Returns:\n",
    "        float: F1 score.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    \n",
    "    if not true_keywords_list and not pred_keywords_list:\n",
    "        return 1.0 # Both empty, perfect score for this case\n",
    "\n",
    "    if not true_keywords_list and pred_keywords_list:\n",
    "        print(\"not true_keywords_list and pred_keywords_list\")\n",
    "        return 0.0 # No true keywords but predictions, all false positives\n",
    "\n",
    "    if true_keywords_list and not pred_keywords_list:\n",
    "        print(\"true_keywords_list and not pred_keywords_list\")\n",
    "        return 0.0 # True keywords but no predictions, all false negatives\n",
    "\n",
    "\n",
    "    #print(f\"\\n--- Embedding Generation Check ---\")\n",
    "    #print(f\"Keywords to embed (True): {true_keywords_list}\")\n",
    "    #print(f\"Keywords to embed (Pred): {pred_keywords_list}\")\n",
    "   \n",
    "\n",
    "\n",
    "    true_keyword_embeddings = []\n",
    "    pred_keyword_embeddings = []\n",
    "\n",
    "    \n",
    "    # Get embeddings for all true and predicted keywords in batches for efficiency\n",
    "    try:\n",
    "        \n",
    "        # Iterate and get embedding for each keyword individually\n",
    "        if true_keywords_list:\n",
    "            #print(f\"Attempting to embed {len(true_keywords_list)} true keywords individually...\")\n",
    "            for kw in true_keywords_list:\n",
    "                true_keyword_embeddings.append(embed_model.get_text_embedding(kw))\n",
    "            #print(f\"Successfully embedded {len(true_keyword_embeddings)} true keywords.\")\n",
    "            # print(f\"First true embedding (first 5 dims): {true_keyword_embeddings[0][:5]}\") # Optional debug\n",
    "\n",
    "        if pred_keywords_list:\n",
    "            #print(f\"Attempting to embed {len(pred_keywords_list)} predicted keywords individually...\")\n",
    "            for kw in pred_keywords_list:\n",
    "                pred_keyword_embeddings.append(embed_model.get_text_embedding(kw))\n",
    "            #print(f\"Successfully embedded {len(pred_keyword_embeddings)} predicted keywords.\")\n",
    "            # print(f\"First pred embedding (first 5 dims): {pred_keyword_embeddings[0][:5]}\") # Optional debug\n",
    "\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings: {e}\")\n",
    "        return 0.0 # Handle cases where embedding fails\n",
    "\n",
    "    # Convert to numpy arrays for cosine_similarity\n",
    "    true_keyword_embeddings = np.array(true_keyword_embeddings)\n",
    "    pred_keyword_embeddings = np.array(pred_keyword_embeddings)\n",
    "\n",
    "    matched_true_indices = [False] * len(true_keywords_list)\n",
    "\n",
    "    for i, pred_emb in enumerate(pred_keyword_embeddings):\n",
    "        found_match = False\n",
    "        best_similarity = -1.0 # Cosine similarity ranges from -1 to 1\n",
    "        best_true_idx = -1\n",
    "\n",
    "        for j, true_emb in enumerate(true_keyword_embeddings):\n",
    "            if not matched_true_indices[j]: # Only consider unmatched true keywords\n",
    "                # Reshape for sklearn.metrics.pairwise.cosine_similarity\n",
    "                # It expects 2D arrays, even for single vectors\n",
    "                sim = cosine_similarity(pred_emb.reshape(1, -1), true_emb.reshape(1, -1))[0][0]\n",
    "\n",
    "                if sim > best_similarity:\n",
    "                    best_similarity = sim\n",
    "                    best_true_idx = j\n",
    "\n",
    "        if best_similarity >= similarity_threshold:\n",
    "            tp += 1\n",
    "            if best_true_idx != -1:\n",
    "                matched_true_indices[best_true_idx] = True\n",
    "            found_match = True\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    fn = sum(1 for is_matched in matched_true_indices if not is_matched)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78627331-e09b-4474-99b5-f79322c48dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedJsonOutputEvaluator(BaseEvaluator):\n",
    "    \"\"\"\n",
    "    An evaluator for checking JSON output correctness, calculating specific F1 scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, name: str = \"EnhancedJsonOutputEvaluator\"):\n",
    "        super().__init__()\n",
    "\n",
    "     # --- ADD THESE ABSTRACT METHOD IMPLEMENTATIONS ---\n",
    "    def _get_prompts(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get prompts for the evaluator. Not directly used in this JSON evaluation context.\"\"\"\n",
    "        # Since this evaluator relies on direct response/reference comparison\n",
    "        # and doesn't generate its own prompts based on abstract methods,\n",
    "        # we return an empty dictionary or a placeholder.\n",
    "        return {}\n",
    "\n",
    "    def _update_prompts(self, prompts_dict: Dict[str, Any]) -> None:\n",
    "        \"\"\"Update prompts for the evaluator. Not directly used in this JSON evaluation context.\"\"\"\n",
    "        # Similarly, no prompts to update from an external source in this evaluator's logic.\n",
    "        pass\n",
    "    # --- END OF ADDED METHODS ---\n",
    "    def _normalize_bool_to_string(self, value: Any) -> Optional[str]:\n",
    "        \"\"\"Normalizes boolean-like values to 'true' or 'false' strings.\"\"\"\n",
    "        if value is True:\n",
    "            return 'true'\n",
    "        if value is False:\n",
    "            return 'false'\n",
    "        if isinstance(value, str):\n",
    "            lower_val = value.strip().lower()\n",
    "            if lower_val == 'true':\n",
    "                return 'true'\n",
    "            if lower_val == 'false':\n",
    "                return 'false'\n",
    "        # Handle cases where value might be None or unexpected\n",
    "        return None \n",
    "\n",
    "    \n",
    "    async def aevaluate(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "        response: Optional[str] = None,\n",
    "        reference: Optional[str] = None, # This will be the target_json_output as a string\n",
    "        **kwargs: Any,\n",
    "    ) -> EvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate the generated JSON response against the reference JSON using F1 scores.\n",
    "        \"\"\"\n",
    "        parsed_llm_output = None\n",
    "        is_valid_json = True\n",
    "        overall_score = 0.0\n",
    "        feedback = []\n",
    "\n",
    "        # 1. Validate JSON structure\n",
    "        try:\n",
    "            # Attempt to parse the LLM's response.\n",
    "            clean_response = response.strip()\n",
    "            if clean_response.startswith(\"```json\") and clean_response.endswith(\"```\"):\n",
    "                clean_response = clean_response[len(\"```json\"): -len(\"```\")].strip()\n",
    "                feedback.append(f\"json needed cleaning.\")\n",
    "            elif clean_response.startswith(\"```\") and clean_response.endswith(\"```\"):\n",
    "                clean_response = clean_response[len(\"```\"): -len(\"```\")].strip()\n",
    "                feedback.append(f\"json needed cleaning.\")\n",
    "            \n",
    "            parsed_llm_output = json.loads(clean_response)\n",
    "        except json.JSONDecodeError as e:\n",
    "            is_valid_json = False\n",
    "            feedback.append(f\"Invalid JSON output: {e}\")\n",
    "            return EvaluationResult(\n",
    "                query=query,\n",
    "                response=response,\n",
    "                passing=False,\n",
    "                score=0.0,\n",
    "                feedback=\"\\n\".join(feedback),\n",
    "                invalid_result=True,\n",
    "                invalid_reason=feedback[0],\n",
    "                eval_f1_overall_sentiment=0.0,\n",
    "                eval_f1_negative_tracker=0.0,\n",
    "                eval_f1_keywords=0.0\n",
    "            )\n",
    "\n",
    "        # If JSON is valid, proceed with F1 score calculations\n",
    "        reference_json = json.loads(reference) # Reference is already parsed by load_jsonl_data\n",
    "\n",
    "        sentiment_f1 = 0.0\n",
    "        negative_tracker_f1 = 0.0\n",
    "        keywords_f1 = 0.0\n",
    "        review_flag_correctness = {}\n",
    "        \n",
    "        # --- Overall Sentiment - Weighted F1 ---\n",
    "        try:\n",
    "            sentiment_true_labels = []\n",
    "            sentiment_pred_labels = []\n",
    "            sentiment_weights = []\n",
    "\n",
    "            # Overall sentiment (higher weight)\n",
    "            true_overall = reference_json.get(\"sentiment\", {}).get(\"overall\")\n",
    "            pred_overall = parsed_llm_output.get(\"sentiment\", {}).get(\"overall\")\n",
    "\n",
    "            # Ensure overall sentiment is always lowercase string\n",
    "            true_overall_norm = true_overall.strip().lower() if isinstance(true_overall, str) else None\n",
    "            pred_overall_norm = pred_overall.strip().lower() if isinstance(pred_overall, str) else None\n",
    "\n",
    "            sentiment_true_labels.append(true_overall_norm)\n",
    "            sentiment_pred_labels.append(pred_overall_norm)\n",
    "            sentiment_weights.append(0.5)\n",
    "\n",
    "            # Recommendation\n",
    "            true_reco = reference_json.get(\"sentiment\", {}).get(\"recommendation\")\n",
    "            pred_reco = parsed_llm_output.get(\"sentiment\", {}).get(\"recommendation\")\n",
    "\n",
    "            true_reco_norm = self._normalize_bool_to_string(true_reco)\n",
    "            pred_reco_norm = self._normalize_bool_to_string(pred_reco)\n",
    "            \n",
    "            sentiment_true_labels.append(true_reco_norm)\n",
    "            sentiment_pred_labels.append(pred_reco_norm)\n",
    "            sentiment_weights.append(0.25)\n",
    "\n",
    "            # Warning anti-recommendation\n",
    "            true_warn = reference_json.get(\"sentiment\", {}).get(\"warning_anti_recommendation\")\n",
    "            pred_warn = parsed_llm_output.get(\"sentiment\", {}).get(\"warning_anti_recommendation\")\n",
    "\n",
    "            true_warn_norm = self._normalize_bool_to_string(true_warn)\n",
    "            pred_warn_norm = self._normalize_bool_to_string(pred_warn)\n",
    "\n",
    "            sentiment_true_labels.append(true_warn_norm)\n",
    "            sentiment_pred_labels.append(pred_warn_norm)\n",
    "            sentiment_weights.append(0.25)\n",
    "\n",
    "            combined_true = []\n",
    "            combined_pred = []\n",
    "            \n",
    "            for t, p, w in zip(sentiment_true_labels, sentiment_pred_labels, sentiment_weights):\n",
    "                if t is not None and p is not None:\n",
    "                    combined_true.append(t)\n",
    "                    combined_pred.append(p)\n",
    "\n",
    "            # Reconstruct combined_weights based on what's actually appended\n",
    "            combined_weights = []\n",
    "            weights_map = {0: 0.5, 1: 0.25, 2: 0.25} \n",
    "            for i, (t, p) in enumerate(zip(sentiment_true_labels, sentiment_pred_labels)):\n",
    "                if t is not None and p is not None:\n",
    "                    combined_weights.append(weights_map.get(i, 0.0))\n",
    "\n",
    "        \n",
    "            \n",
    "            \n",
    "            if combined_true:\n",
    "                ALL_SENTIMENT_F1_LABELS = ['positive', 'negative', 'neutral', 'mixed', 'true', 'false']\n",
    "\n",
    "                # TEST -----------------------------------------------\n",
    "                per_class_f1 = f1_score(combined_true, combined_pred,\n",
    "                                        labels=ALL_SENTIMENT_F1_LABELS,\n",
    "                                        average=None, # Get F1 for each class\n",
    "                                        zero_division=0)\n",
    "                #print(f\"DEBUG: Sentiment Per-Class F1 Scores: {per_class_f1}\")\n",
    "                #print(f\"DEBUG: Sentiment Labels considered by f1_score: {f1_score(combined_true, combined_pred, average=None, zero_division=0, return_indices_as_labels=True)}\") # This helps map scores to labels\n",
    "\n",
    "                # Manual weighted average calculation (to mimic 'weighted')\n",
    "                # Need to get class counts (support)\n",
    "                from collections import Counter\n",
    "                true_label_counts = Counter(combined_true)\n",
    "                \n",
    "                weighted_f1_manual = 0.0\n",
    "                total_support = len(combined_true)\n",
    "                if total_support > 0:\n",
    "                    for i, label in enumerate(ALL_SENTIMENT_F1_LABELS):\n",
    "                        # Find the index of the label in the f1_score's own ordered labels\n",
    "                        # This part needs careful handling if labels are reordered by f1_score\n",
    "                        # Simpler: just iterate over the labels provided by f1_score when average=None\n",
    "                        pass # We will rely on per_class_f1 and the order of labels returned by f1_score.\n",
    "\n",
    "                # Let's try average='micro' or 'accuracy_score' as a sanity check.\n",
    "                from sklearn.metrics import accuracy_score\n",
    "                accuracy_sentiment = accuracy_score(combined_true, combined_pred)\n",
    "                #print(f\"DEBUG: Sentiment Accuracy Score: {accuracy_sentiment}\")\n",
    "\n",
    "                # TEST -----------------------------------------------\n",
    "                \n",
    "                sentiment_f1 = f1_score(combined_true, combined_pred, \n",
    "                                        labels=ALL_SENTIMENT_F1_LABELS, # Pass explicit labels\n",
    "                                        average='weighted', zero_division=0)\n",
    "            else:\n",
    "                sentiment_f1 = 0.0\n",
    "                \n",
    "            #print(f\"Final calculated sentiment_f1: {sentiment_f1}\") \n",
    "            feedback.append(f\"Overall Sentiment F1: {sentiment_f1:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            feedback.append(f\"Error calculating sentiment F1: {e}\")\n",
    "            sentiment_f1 = 0.0\n",
    "            print(f\"DEBUG: Exception caught for sentiment F1: {e}\")\n",
    "\n",
    "        # --- Negative Tracker Flags - Macro F1 ---\n",
    "        try:\n",
    "            negative_flags = [\n",
    "                'ad_game_mismatch', 'game_cheating_manipulating',\n",
    "                'bugs_crashes_performance', 'monetization', 'live_ops_events'\n",
    "            ]\n",
    "            \n",
    "            true_labels_nt = []\n",
    "            pred_labels_nt = []\n",
    "\n",
    "            for flag in negative_flags:\n",
    "                true_val = reference_json.get(\"negative_tracker\", {}).get(flag, False)\n",
    "                pred_val = parsed_llm_output.get(\"negative_tracker\", {}).get(flag, False)\n",
    "\n",
    "                # Check if the predicted value matches the true value\n",
    "                is_correct = (self._normalize_bool_to_string(true_val) == self._normalize_bool_to_string(pred_val))\n",
    "                review_flag_correctness[f'nt_{flag}_correct'] = is_correct\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "                true_labels_nt.append(int(self._normalize_bool_to_string(true_val) == 'true'))\n",
    "                pred_labels_nt.append(int(self._normalize_bool_to_string(pred_val) == 'true'))\n",
    "            \n",
    "            if true_labels_nt:\n",
    "                negative_tracker_f1 = accuracy_score(true_labels_nt, pred_labels_nt)\n",
    "\n",
    "            \"\"\"\n",
    "                #negative_tracker_f1 = f1_score(true_labels_nt, pred_labels_nt, average='macro', zero_division=0)\n",
    "            else:\n",
    "                negative_tracker_f1 = 0.0\n",
    "\n",
    "            feedback.append(f\"Negative Tracker Macro F1: {negative_tracker_f1:.2f}\")\n",
    "            \"\"\"\n",
    "\n",
    "        except Exception as e:\n",
    "            #feedback.append(f\"Error calculating negative tracker F1: {e}\")\n",
    "            feedback.append(f\"Error calculating negative tracker: {e}\")\n",
    "\n",
    "            #negative_tracker_f1 = 0.0\n",
    "\n",
    "        # --- Keyword Extraction - F1 based on Set Matching ---\n",
    "        try:\n",
    "    \n",
    "            true_pos_keywords = set(reference_json.get(\"specifics\", {}).get(\"positive_keywords\", []))\n",
    "            pred_pos_keywords = set(parsed_llm_output.get(\"specifics\", {}).get(\"positive_keywords\", []))\n",
    "            \n",
    "            true_neg_keywords = set(reference_json.get(\"specifics\", {}).get(\"negative_keywords\", []))\n",
    "            pred_neg_keywords = set(parsed_llm_output.get(\"specifics\", {}).get(\"negative_keywords\", []))\n",
    "\n",
    "            embedding_similarity_threshold = 0.75\n",
    "            \n",
    "            #similarity scoring\n",
    "            #print(f\"DEBUG: Calling f1_pos function with {len(true_pos_keywords)} true and {len(pred_pos_keywords)} pred pos keywords.\")\n",
    "    \n",
    "            f1_pos = calculate_f1_for_keywords_embedding_lenient(\n",
    "                true_pos_keywords,\n",
    "                pred_pos_keywords,\n",
    "                embed_model,\n",
    "                embedding_similarity_threshold\n",
    "            )\n",
    "            #print(f\"DEBUG: f1_pos returned: {f1_pos:.2f}\")\n",
    "\n",
    "            #print(f\"DEBUG: Calling f1_neg function with {len(true_neg_keywords)} true and {len(pred_neg_keywords)} pred neg keywords.\")\n",
    "   \n",
    "            f1_neg = calculate_f1_for_keywords_embedding_lenient(\n",
    "                true_neg_keywords,\n",
    "                pred_neg_keywords,\n",
    "                embed_model, \n",
    "                embedding_similarity_threshold\n",
    "            )\n",
    "            #print(f\"DEBUG: f1_neg returned: {f1_neg:.2f}\")\n",
    "            \n",
    "            #strict scoring\n",
    "            \"\"\"\n",
    "            tp_pos = len(true_pos_keywords.intersection(pred_pos_keywords))\n",
    "            fp_pos = len(pred_pos_keywords - true_pos_keywords)\n",
    "            fn_pos = len(true_pos_keywords - pred_pos_keywords)\n",
    "            \n",
    "            precision_pos = tp_pos / (tp_pos + fp_pos) if (tp_pos + fp_pos) > 0 else 0.0\n",
    "            recall_pos = tp_pos / (tp_pos + fn_pos) if (tp_pos + fn_pos) > 0 else 0.0\n",
    "            f1_pos = (2 * precision_pos * recall_pos) / (precision_pos + recall_pos) if (precision_pos + recall_pos) > 0 else 0.0\n",
    "            \n",
    "            tp_neg = len(true_neg_keywords.intersection(pred_neg_keywords))\n",
    "            fp_neg = len(pred_neg_keywords - true_neg_keywords)\n",
    "            fn_neg = len(true_neg_keywords - pred_neg_keywords)\n",
    "\n",
    "            precision_neg = tp_neg / (tp_neg + fp_neg) if (tp_neg + fp_neg) > 0 else 0.0\n",
    "            recall_neg = tp_neg / (tp_neg + fn_neg) if (tp_neg + fn_neg) > 0 else 0.0\n",
    "            f1_neg = (2 * precision_neg * recall_neg) / (precision_neg + recall_neg) if (precision_neg + recall_neg) > 0 else 0.0\n",
    "            \"\"\"\n",
    "\n",
    "            \n",
    "            keywords_f1 = np.mean([f1_pos, f1_neg]) if (f1_pos is not None and f1_neg is not None) else 0.0\n",
    "            #print(f\"DEBUG: Combined keywords_f1: {keywords_f1:.2f}\") \n",
    "\n",
    "\n",
    "            \n",
    "            feedback.append(f\"Keyword Extraction F1 (Set Matching): {keywords_f1:.2f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating keyword F1: {e}\")\n",
    "            feedback.append(f\"Error calculating keyword F1: {e}\")\n",
    "            keywords_f1 = 0.0\n",
    "\n",
    "        overall_score = (\n",
    "            0.4 * sentiment_f1 +\n",
    "            0.3 * negative_tracker_f1 +\n",
    "            0.3 * keywords_f1\n",
    "        )\n",
    "        feedback.append(f\"Combined Weighted F1: {overall_score:.2f}\")\n",
    "        print(f'Score: {overall_score}')\n",
    "\n",
    "        eval_result = EvaluationResult(\n",
    "            query=query,\n",
    "            response=response,\n",
    "            passing=is_valid_json and (overall_score > 0.7),\n",
    "            score=overall_score,\n",
    "            feedback=\"\\n\".join(feedback),\n",
    "            invalid_result=not is_valid_json,\n",
    "            invalid_reason=feedback[0] if not is_valid_json else None,\n",
    "        )   \n",
    "        return eval_result, sentiment_f1, keywords_f1, review_flag_correctness\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbcf8f0-3078-4726-ac4b-c0efa292b54e",
   "metadata": {},
   "source": [
    "# Running a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc43b5-91d2-41bd-8174-9a7b10665d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_structured_evaluation(\n",
    "    test_data: List[Dict],\n",
    "    llm_callable: Callable[[List[ChatMessage]], Awaitable[ChatResponse]], # Changed type hint\n",
    "    model_name_for_logging: str = \"LLM\",\n",
    "    num_tests_to_run: Optional[int] = None, # New parameter\n",
    "    DISPLAY_RESULTS = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs evaluation for structured JSON output using a given LLM callable function.\n",
    "    \"\"\"\n",
    "    evaluator = EnhancedJsonOutputEvaluator()\n",
    "    results = []\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Starting Evaluation for Model: {model_name_for_logging} ---\")\n",
    "\n",
    "    # Limit the number of tests if num_tests_to_run is specified\n",
    "    data_to_evaluate = test_data[:num_tests_to_run] if num_tests_to_run is not None else test_data\n",
    "\n",
    "    for i, item in enumerate(data_to_evaluate): # Iterate over the limited data\n",
    "        print(f\"\\n--- Evaluating Test Case {i+1}/{len(data_to_evaluate)} ---\")\n",
    "        user_review = item[\"user_review\"]\n",
    "        target_json_output = item[\"target_json_output\"]\n",
    "        # system_prompt = item[\"system_prompt\"] # Not directly used here, but part of messages\n",
    "\n",
    "        messages = create_chat_messages(item)\n",
    "\n",
    "        llm_output_text = \"\"\n",
    "        try:\n",
    "            if DISPLAY_RESULTS:\n",
    "                print(f\"Review: {user_review[:100]}...\")\n",
    "                print(f\"Target JSON: {json.dumps(target_json_output, indent=2)}\")\n",
    "\n",
    "            # Call the passed llm_callable directly\n",
    "            chat_response = llm_callable(messages, model_name_for_logging)\n",
    "            #llm_output_text = chat_response.message.content.strip()\n",
    "            llm_output_text = chat_response.strip()\n",
    "            #llm_output_text = chat_response\n",
    "            \n",
    "            if DISPLAY_RESULTS:\n",
    "                print(f\"LLM Raw Output: {llm_output_text}\")\n",
    "\n",
    "            eval_result, sentiment_f1, keywords_f1, review_flag_correctness = await evaluator.aevaluate(\n",
    "                query=user_review,\n",
    "                response=llm_output_text,\n",
    "                reference=json.dumps(target_json_output)\n",
    "            )\n",
    "\n",
    "           \n",
    "            result_entry = {\n",
    "                \"LLM\": model_name_for_logging,\n",
    "                \"test_case_id\": i,\n",
    "                \"user_review\": user_review,\n",
    "                \"target_json\": target_json_output,\n",
    "                \"llm_raw_output\": llm_output_text,\n",
    "                \"is_valid_json\": not eval_result.invalid_result,\n",
    "                \"score\": eval_result.score,\n",
    "                \"feedback\": eval_result.feedback,\n",
    "                \"f1_overall_sentiment\": sentiment_f1,\n",
    "                \"f1_keywords\": keywords_f1,\n",
    "                \"nt_ad_game_mismatch_correct\":review_flag_correctness[\"nt_ad_game_mismatch_correct\"],\n",
    "                \"nt_game_cheating_manipulating_correct\":review_flag_correctness[\"nt_game_cheating_manipulating_correct\"],\n",
    "                \"nt_bugs_crashes_performance_correct\":review_flag_correctness[\"nt_bugs_crashes_performance_correct\"],\n",
    "                \"nt_monetization_correct\":review_flag_correctness[\"nt_monetization_correct\"],\n",
    "                \"nt_live_ops_events_correct\":review_flag_correctness[\"nt_live_ops_events_correct\"],\n",
    "            }\n",
    "            \n",
    "            results.append(result_entry)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing test case {i}: {e}\")\n",
    "            results.append({\n",
    "                \"LLM\": model_name_for_logging,\n",
    "                \"test_case_id\": i,\n",
    "                \"user_review\": user_review,\n",
    "                \"target_json\": target_json_output,\n",
    "                \"llm_raw_output\": llm_output_text if llm_output_text else f\"Error: {e}\",\n",
    "                \"is_valid_json\": False,\n",
    "                \"score\": 0.0,\n",
    "                \"feedback\": f\"Runtime Error: {e}\",\n",
    "                \"f1_overall_sentiment\": 0.0,\n",
    "                \"f1_keywords\": 0.0,\n",
    "                \"nt_ad_game_mismatch_correct\":0.0,\n",
    "                \"nt_game_cheating_manipulating_correct\":0.0,\n",
    "                \"nt_bugs_crashes_performance_correct\":0.0,\n",
    "                \"nt_monetization_correct\":0.0,\n",
    "                \"nt_live_ops_events_correct\":0.0,\n",
    "            })\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90ee0b-55da-48c2-a9dd-048c1e5c7809",
   "metadata": {},
   "source": [
    "# Final scoring of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d22bc-2d4b-4e25-87f3-a5bbdf20097b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. negative tracker scoring\n",
    "#2. final overall score\n",
    "def calculate_final_metrics(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Calculates final aggregated metrics from a DataFrame of test results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing per-test-case results with\n",
    "                           individual negative flag correctness.\n",
    "                           Expected columns:\n",
    "                           - 'is_valid_json' (bool)\n",
    "                           - 'f1_overall_sentiment' (float)\n",
    "                           - 'f1_keywords' (float)\n",
    "                           - 'nt_ad_game_mismatch_correct' (bool)\n",
    "                           - 'nt_game_cheating_manipulating_correct' (bool)\n",
    "                           - 'nt_bugs_crashes_performance_correct' (bool)\n",
    "                           - 'nt_monetization_correct' (bool)\n",
    "                           - 'nt_live_ops_events_correct' (bool)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the final overall score and F1 scores.\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Warning: Input DataFrame is empty. Returning default zero metrics.\")\n",
    "        return {\n",
    "            'final_overall_score': 0.0,\n",
    "            'final_f1_overall_sentiment': 0.0,\n",
    "            'final_f1_negative_tracker': 0.0,\n",
    "            'final_f1_keywords': 0.0\n",
    "        }\n",
    "\n",
    "    # Calculate overall F1 for Sentiment\n",
    " \n",
    "    final_f1_overall_sentiment = df['f1_overall_sentiment'].mean()\n",
    "\n",
    "    # Calculate overall F1 for Keywords\n",
    "    final_f1_keywords = df['f1_keywords'].mean()\n",
    "\n",
    "    # Calculate overall F1 for Negative Trackers\n",
    "    negative_flag_columns = [\n",
    "        'nt_ad_game_mismatch_correct',\n",
    "        'nt_game_cheating_manipulating_correct',\n",
    "        'nt_bugs_crashes_performance_correct',\n",
    "        'nt_monetization_correct',\n",
    "        'nt_live_ops_events_correct'\n",
    "    ]\n",
    "\n",
    "    # Flatten all correctness flags into a single series\n",
    "    all_nt_correctness = df[negative_flag_columns].values.flatten()\n",
    "\n",
    "    final_f1_negative_tracker = np.mean(all_nt_correctness) if len(all_nt_correctness) > 0 else 0.0\n",
    "\n",
    "    # If you *did* have the original true and predicted 0/1 values stored in the DataFrame\n",
    "    # (e.g., 'nt_ad_game_mismatch_true', 'nt_ad_game_mismatch_pred'),\n",
    "    # then the calculation would be:\n",
    "    # all_true_nt_labels = df[[col.replace('_correct', '_true') for col in negative_flag_columns]].values.flatten()\n",
    "    # all_pred_nt_labels = df[[col.replace('_correct', '_pred') for col in negative_flag_columns]].values.flatten()\n",
    "    # final_f1_negative_tracker = f1_score(all_true_nt_labels, all_pred_nt_labels, average='micro', zero_division=0)\n",
    "    #\n",
    "    # For the current DataFrame structure, `np.mean(all_nt_correctness)` is the most sensible\n",
    "    # interpretation that aligns with \"accuracy\" and maps to micro F1 for this scenario.\n",
    "\n",
    "    # Calculate the Overall Score\n",
    "    # weights: .4*sentiment f1, .3* negative tracker f1, .3 * keywords f1\n",
    "    sentiment_weight = 0.4\n",
    "    negative_tracker_weight = 0.3\n",
    "    keywords_weight = 0.3\n",
    "\n",
    "    final_overall_score = (\n",
    "        sentiment_weight * final_f1_overall_sentiment +\n",
    "        negative_tracker_weight * final_f1_negative_tracker +\n",
    "        keywords_weight * final_f1_keywords\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'final_overall_score': final_overall_score,\n",
    "        'final_f1_overall_sentiment': final_f1_overall_sentiment,\n",
    "        'final_f1_negative_tracker': final_f1_negative_tracker, # This is micro-F1/accuracy\n",
    "        'final_f1_keywords': final_f1_keywords\n",
    "    }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8cc68-2857-412a-9c9e-55df7855b8cb",
   "metadata": {},
   "source": [
    "# Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e884f-58e1-48c3-823d-468fe4b9f98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA ----------------------------------------------\n",
    "\n",
    "jsonl_file_path = \"test_data_NEW.jsonl\"\n",
    "test_data_structured = load_my_jsonl_data(jsonl_file_path)\n",
    "print(f'Loaded {len(test_data_structured)} test cases.')\n",
    "#print(\"Example data:\")\n",
    "#print(test_data_structured[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3141f-3896-4089-993d-6db16631b477",
   "metadata": {},
   "source": [
    "# TESTING MY CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f17865-8a10-42d1-b7e7-45483f203fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"llama3.1:8b\"  \n",
    "gemini_model = 'models/gemini-2.0-flash-lite'\n",
    "test_model = \"hf.co/MrMike42/GameReview-llama3.1-8b-v9-Q4_K_M-GGUF:latest\"\n",
    "num_tests = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939f49e-5733-4b11-898c-caf4f847b6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856a116-805a-4e1e-a4e6-bbbdb68890de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#GEMINI\n",
    "#test_results_df = await run_structured_evaluation(test_data_structured, ask_gemini_json, gemini_model, num_tests)\n",
    "\n",
    "#LLAMA3\n",
    "#test_results_df = await run_structured_evaluation(test_data_structured, ask_llama3_json, base_model, num_tests)\n",
    "\n",
    "#FINETUNE\n",
    "test_results_df = await run_structured_evaluation(test_data_structured, ask_llama3_json, test_model, num_tests, True)\n",
    "\n",
    "\n",
    "results_df = test_results_df\n",
    "\n",
    "test_final_metrics = calculate_final_metrics(results_df)\n",
    "print('Results data:')\n",
    "print(results_df[['test_case_id', 'is_valid_json',  'f1_overall_sentiment',  'f1_keywords']])\n",
    "print(f'-----\\nFinal results:\\n')\n",
    "print(test_final_metrics)\n",
    "test_results_df.to_csv(\"test9_results.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d7f30-6a07-471f-9370-cb354b31a46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
